# Wyzer AI Assistant - System Flow Diagram

**Related Documents:**
- [REPO_STRUCTURE.md](REPO_STRUCTURE.md) - Complete repository tree and module descriptions
- [TOOL_CHAIN.md](TOOL_CHAIN.md) - Available tools and tool execution pipeline
- [THREE_PROCESS_ARCHITECTURE.md](THREE_PROCESS_ARCHITECTURE.md) - Multiprocess design details

---

## High-Level Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    WYZER AI ASSISTANT v2                        │
│                  (Multiprocess Architecture)                    │
└─────────────────────────────────────────────────────────────────┘

┌──────────────────────────┐         ┌──────────────────────────┐
│   PROCESS A: CORE        │         │   PROCESS B: BRAIN       │
│   (Realtime Reactive)    │         │   (Heavy Processing)     │
│                          │         │                          │
│ • Microphone input       │         │ • STT (Whisper)          │
│ • Hotword detection      │◄───────►│ • LLM (Ollama)           │
│ • VAD (Voice Activity)   │ IPC Q's │ • Tool execution         │
│ • State machine          │         │ • TTS (Piper)            │
│ • Barge-in handling      │         │ • Followup logic         │
│ • FOLLOWUP window        │         │                          │
└──────────────────────────┘         └──────────────────────────┘
```

---

## Complete User Interaction Flow

### Phase 1: IDLE → LISTENING (Hotword Detection)

```
START (IDLE state)
    ↓
Microphone stream running (continuous)
    ↓
Hotword detector listening for "hey jarvis" (in IDLE)
    ↓
[Hotword detected?]
    ├─ NO: Continue listening
    └─ YES:
        ↓
        Transition to LISTENING state
        ↓
        Start collecting audio into buffer
        ↓
        Enable barge-in (listen for hotword to interrupt TTS)
```

### Phase 2: LISTENING → TRANSCRIBING (Audio Collection)

```
LISTENING state (audio being recorded)
    ↓
VAD detector monitors audio:
    ├─ No speech? → Keep listening
    ├─ Speech detected? → Reset silence timer
    └─ Silence for N seconds?
        ↓
        END audio collection
        ↓
        Transition to TRANSCRIBING state
        ↓
        Send audio to Brain Worker (STT)
```

### Phase 3: TRANSCRIBING → THINKING (Speech-to-Text)

```
TRANSCRIBING state (Brain Worker receives audio)
    ↓
STT Router determines which engine to use
    ├─ Whisper (default, faster-whisper)
    └─ Other engines...
    ↓
Whisper transcribes audio → text transcript
    ↓
Check for spam/garbage:
    ├─ Repetition detected? → Filter it
    └─ Valid speech?
        ↓
        Core receives transcript
        ↓
        Transition to THINKING state
        ↓
        Log: "[TRANSCRIPT] User said: '...'"
```

### Phase 4: THINKING (Intent Routing & Tool Selection)

```
THINKING state (Brain Worker processes intent)
    ↓
Hybrid Router analyzes transcript:
    │
    ├─ Path A: DETERMINISTIC ROUTE (70% of commands)
    │   ├─ Is it a "simple" command? (volume, media, window, etc.)
    │   ├─ Parse intent deterministically (no LLM)
    │   ├─ Extract arguments (e.g., volume level)
    │   └─ Execute tool immediately
    │       ↓
    │       Return result → SPEAKING state
    │
    └─ Path B: LLM ROUTE (30% of commands)
        ├─ Ambiguous or conversational query?
        ├─ Send to LLM:
        │   ├─ System prompt: "You are Wyzer, a voice assistant..."
        │   ├─ User: "[transcript]"
        │   ├─ LLM can call tools via function calling
        │   └─ LLM generates response
        │
        ├─ LLM Tool Calling Loop:
        │   ├─ [Tool needed?]
        │   │   ├─ NO: Skip to response
        │   │   └─ YES: Execute tool
        │   │       ├─ Validate arguments against JSON schema
        │   │       ├─ Run tool (e.g., get_weather, open_website)
        │   │       ├─ Tool returns result (JSON)
        │   │       └─ Send result back to LLM
        │   │           ├─ [Need more tools?] → Loop
        │   │           └─ [Done?] → Generate response
        │   │
        │   └─ Final response generated by LLM
        │
        └─ Return response → SPEAKING state
```

### Phase 5: SPEAKING → FOLLOWUP (Text-to-Speech)

```
SPEAKING state (TTS rendering)
    ↓
TTS Router determines engine:
    ├─ Piper (default, local, fast)
    └─ Other engines...
    ↓
TTS generates audio from response text
    │
    ├─ Barge-in Enabled? 
    │   ├─ YES: Listen for hotword during TTS playback
    │   │   ├─ [Hotword detected during playback?]
    │   │   │   ├─ YES: Stop TTS, return to LISTENING
    │   │   │   └─ NO: Continue
    │   └─ NO: Just play audio
    │
    ├─ Play audio on selected output device
    └─ [TTS playback complete?]
        ↓
        Check FOLLOWUP manager
```

### Phase 6: FOLLOWUP Listening Window

```
[After TTS completes]
    ↓
[FOLLOWUP enabled?]
    ├─ NO: Return to IDLE state
    └─ YES:
        ↓
        FOLLOWUP state: hotword disabled, listening for follow-up
        ├─ Timeout: ~3 seconds of silence
        ├─ Early exit phrases: "no", "nope", "stop", etc.
        └─ Other text: Treat as new query
        ↓
        [User speaks?]
        ├─ NO (silence for 3s): Exit FOLLOWUP → IDLE
        ├─ YES (exit phrase): Exit FOLLOWUP → IDLE
        └─ YES (new query):
            ↓
            Reset speech timer, continue FOLLOWUP
            ↓
            Transcribe new query (no hotword needed)
            ↓
            Back to THINKING state (Phase 4)
            ↓
            LLM processes follow-up
            ↓
            TTS responds
            ↓
            Re-enter FOLLOWUP (chain multiple follow-ups)
            ↓
            [Continue until exit phrase or timeout]
```

---

## Detailed Phase 4: Hybrid Router Decision Tree

```
INPUT: User transcript
    ↓
┌─ DETERMINISTIC FAST PATH? ───────────────────────┐
│                                                   │
├─ Pattern: Volume commands                        │
│  └─ "volume 50", "turn up volume", etc.          │
│     └─ Tool: volume_control                      │
│                                                   │
├─ Pattern: Media controls                         │
│  └─ "play", "pause", "next track", etc.          │
│     └─ Tool: media_play_pause, media_next_track  │
│                                                   │
├─ Pattern: Window management                      │
│  └─ "minimize", "maximize", "focus window", etc. │
│     └─ Tool: window_manager                      │
│                                                   │
├─ Pattern: Open/launch                            │
│  └─ "open downloads", "launch chrome", etc.      │
│     └─ Tool: open_target, open_website           │
│                                                   │
├─ Pattern: Simple queries                         │
│  └─ "what time is it?", "current location", etc.│
│     └─ Tool: get_time, get_location              │
│                                                   │
└─ MATCH FOUND? ───────────────────────────────────┘
    │
    ├─ YES: Execute tool directly (no LLM)
    │   └─ Return result
    │
    └─ NO: Ambiguous/conversational → LLM ROUTE
        ↓
        Send to Ollama LLM with:
        ├─ System prompt (role definition)
        ├─ Tool definitions (function schema)
        ├─ Conversation history
        └─ User query
        ↓
        LLM responds with:
        ├─ Text response, OR
        ├─ Function call request, OR
        ├─ Multiple function calls
        ↓
        [Need tools?]
        ├─ YES: Execute tools, send results back to LLM
        └─ NO: Return text response
```

---

## Tool Execution Pipeline (Phase 4 Detail)

```
[LLM decides to use tool]
    ↓
Tool Name: "volume_control"
Tool Args: {"action": "set_master", "volume": 50}
    ↓
VALIDATION STEP:
    ├─ Check JSON schema
    ├─ Validate argument types
    ├─ Ensure required args present
    └─ [Valid?]
        ├─ NO: Return error
        └─ YES: Proceed
    ↓
EXECUTION STEP:
    ├─ Load tool from registry
    ├─ Instantiate tool class
    ├─ Call tool.run(**args)
    └─ Tool executes (e.g., adjust Windows volume)
    ↓
RESULT STEP:
    ├─ Tool returns JSON result
    ├─ Example: {"volume": 50, "is_muted": false}
    └─ Send result back to LLM
    ↓
LLM processes result:
    ├─ [Need more tools?] → Loop back
    └─ [Done?] → Generate response
```

---

## State Machine Overview

```
States and Transitions:

IDLE
 ├─ Hotword detected? → LISTENING
 └─ Stay IDLE

LISTENING (collecting audio after hotword)
 ├─ Barge-in (hotword during TTS)? → Interrupt & LISTENING
 ├─ Silence detected (VAD)? → TRANSCRIBING
 └─ Timeout? → IDLE

TRANSCRIBING (STT in progress)
 ├─ Transcription complete? → THINKING
 └─ Error? → IDLE

THINKING (LLM + tool execution)
 ├─ Response ready? → SPEAKING
 └─ Error? → IDLE

SPEAKING (TTS in progress)
 ├─ Barge-in (hotword)? → LISTENING
 ├─ TTS complete, FOLLOWUP enabled? → FOLLOWUP
 └─ TTS complete, FOLLOWUP disabled? → IDLE

FOLLOWUP (hotword disabled, listening for follow-ups)
 ├─ User speaks (no hotword needed) → TRANSCRIBING (then THINKING)
 ├─ Exit phrase detected? → IDLE
 ├─ Timeout (silence 3s)? → IDLE
 └─ User response leads back to SPEAKING
```

---

## IPC (Inter-Process Communication)

```
Core Process ◄─────────────────────────► Brain Worker Process
             │                          │
             ├─ core_to_brain_q         │
             │  (Core → Worker)         │
             │  - Audio WAV file path   │
             │  - Config updates        │
             │  - Commands              │
             │                          │
             └─ brain_to_core_q         │
                (Worker → Core)         │
                - Transcripts           │
                - LLM responses         │
                - Tool results          │
                - Logs/telemetry        │
                - TTS audio ready       │

Messages are JSON-encoded with:
├─ Request ID (correlate responses)
├─ Type (e.g., "transcribe", "llm_request")
├─ Payload (actual data)
└─ Timestamp
```

---

## Tool Registry & Execution

```
Available Tools:

System Information:
├─ get_time: Current time/date
├─ get_system_info: OS, CPU, RAM
├─ get_location: IP-based geolocation
├─ monitor_info: Connected monitors

Media & Audio:
├─ media_play_pause: Toggle playback
├─ media_next_track: Skip track
├─ media_previous_track: Previous track
├─ volume_control: Master & per-app volume
├─ audio_output_device: Switch speakers/headphones

Window Management:
├─ window_manager: List, focus, minimize, move windows

Opening/Launching:
├─ open_target: Open folders, files, apps
├─ open_website: Open websites in browser
├─ local_library_refresh: Rebuild app index

Weather:
├─ get_weather_forecast: Weather + 7-day forecast

All tools:
├─ Inherit from ToolBase
├─ Define JSON schema for arguments
├─ Return JSON results
├─ Support error handling
└─ Stateless (no side effects between calls)
```

---

## Example: "Play Spotify and set volume to 75%"

```
1. USER SPEAKS: "Play Spotify and set volume to 75%"

2. HOTWORD DETECTED → LISTENING

3. AUDIO COLLECTED → VAD detects speech end

4. STT TRANSCRIPTION → "Play Spotify and set volume to 75%"

5. HYBRID ROUTER ANALYSIS:
   ├─ Multiple intents detected
   ├─ Not a pure deterministic command
   └─ Route to LLM

6. LLM PROCESSES:
   ├─ System prompt: "You are Wyzer..."
   ├─ User: "Play Spotify and set volume to 75%"
   ├─ LLM recognizes two tools needed:
   │  ├─ Tool 1: open_target (query: "spotify")
   │  └─ Tool 2: volume_control (set to 75%)
   │
   └─ LLM function calling:
      ├─ First call: open_target(query="spotify")
      │  └─ Returns: {"status": "ok", "target": "C:\\Program Files\\Spotify\\..."}
      ├─ Second call: volume_control(action="set_master", volume=75)
      │  └─ Returns: {"volume": 75, "is_muted": false}
      └─ LLM generates response: "Opened Spotify and set volume to 75%"

7. TTS GENERATION:
   ├─ Piper converts: "Opened Spotify and set volume to 75%"
   └─ Generates audio file

8. AUDIO PLAYBACK:
   ├─ Play through default device
   ├─ Barge-in enabled (listen for hotword)
   └─ [TTS complete]

9. FOLLOWUP WINDOW OPENS:
   ├─ "What should I play?" (no hotword needed)
   ├─ Transcribed and sent to LLM
   ├─ LLM responds: "Here are some popular playlists..."
   ├─ TTS generates response
   ├─ Re-enter FOLLOWUP
   └─ [Continue until "that's all" or 3s silence]
```

---

## Multi-Intent Extraction (Phase 6)

```
INPUT: "Open downloads, play some music, and set volume to 50"

ORCHESTRATOR ANALYSIS:
├─ Attempt FASTPATH split on conjunctions
│  ├─ Split: "Open downloads" | "play some music" | "set volume to 50"
│  │
│  └─ For each clause:
│      ├─ "Open downloads"
│      │  └─ Matches deterministic: open_target
│      ├─ "play some music"
│      │  └─ Needs LLM (conversational)
│      └─ "set volume to 50"
│         └─ Matches deterministic: volume_control
│
├─ Send to LLM (if mixed):
│  ├─ Instruction: "Process as a sequence of intents"
│  ├─ LLM determines order and dependencies
│  └─ Return: [Intent 1, Intent 2, Intent 3]
│
└─ EXECUTION:
   ├─ Intent 1: open_target(query="downloads") → executes
   ├─ Intent 2: [depends on LLM] → may use tools
   ├─ Intent 3: volume_control(action="set_master", volume=50) → executes
   └─ Generate combined response

RESPONSE: "Opened downloads folder, started playing music, set volume to 50"
```

---

## Error Handling & Recovery

```
[Tool Execution Error]
    ↓
Tool returns: {"error": {"type": "...", "message": "..."}}
    ↓
[Error type?]
    ├─ invalid_argument: LLM retries with corrected args
    ├─ execution_error: LLM generates error explanation
    ├─ permission_error: Explain to user ("Need admin rights")
    └─ platform_error: Skip tool, use alternative
    ↓
LLM generates response explaining what went wrong
    ↓
TTS responds with explanation
    ↓
FOLLOWUP opens (allow retry)
```

---

## Performance Optimizations

```
1. DETERMINISTIC FASTPATH (70% of commands)
   ├─ Skip LLM entirely
   ├─ Direct to tool execution
   └─ Sub-200ms response time

2. MULTIPROCESS SPLIT
   ├─ Core stays responsive during heavy ops
   ├─ STT, LLM, TTS run in background
   └─ Don't block audio/hotword detection

3. FOLLOWUP NO-HOTWORD
   ├─ Skip hotword detection for 3 seconds
   ├─ Deterministic silence detection
   ├─ Chain multiple queries without re-hotword
   └─ Feels more conversational

4. BARGE-IN ASYNC
   ├─ Check for hotword during TTS
   ├─ Interrupt playback immediately
   └─ Don't wait for TTS to finish

5. WINDOW CACHE (300s TTL)
   ├─ Cache window handles for fast lookup
   ├─ Avoid repeated API calls
   └─ Graceful TTL expiry
```

---

## Configuration & Logging

```
Core Config (Config class):
├─ SAMPLE_RATE: 16000 Hz
├─ CHUNK_SIZE: Samples per buffer
├─ VAD_THRESHOLD: Speech detection sensitivity
├─ HOTWORD_THRESHOLD: How confident for "hey jarvis"
├─ FOLLOWUP_ENABLED: Enable follow-up window
├─ FOLLOWUP_TIMEOUT_SEC: Duration (default 3s)
├─ OLLAMA_BASE_URL: LLM endpoint (default localhost:11434)
├─ OLLAMA_MODEL: Model name
├─ LLM_TIMEOUT: Max wait time for LLM response
├─ LOG_LEVEL: DEBUG, INFO, WARN, ERROR
└─ ... and more

Logger:
├─ [HOTWORD] hotword detection events
├─ [VAD] voice activity detection
├─ [TRANSCRIPT] transcription results
├─ [LLM] LLM requests/responses
├─ [TOOL] tool calls and results
├─ [FOLLOWUP] follow-up window events
├─ [STATE] state transitions
└─ [ERROR] errors and exceptions
```

---

## Barge-in (Interrupt TTS) Mechanism

```
[TTS PLAYBACK STARTED]
    ↓
Background thread: Listen for hotword
    ├─ [Hotword detected?]
    │   ├─ YES:
    │   │  ├─ Stop TTS immediately
    │   │  ├─ Discard remaining audio
    │   │  ├─ Set barge-in flag
    │   │  └─ Return to LISTENING
    │   │
    │   └─ NO: Continue playback
    │
    └─ [Playback finished normally?]
        ├─ YES: Enter FOLLOWUP
        └─ NO: Already interrupted
```

---

## Summary

This multiprocess architecture achieves:
✅ **Responsive core**: Always listening, always responsive  
✅ **Powerful brain**: LLM + tools run without blocking audio  
✅ **Natural conversation**: FOLLOWUP window enables chaining  
✅ **Deterministic shortcuts**: 70% of commands skip LLM  
✅ **Safe tool calling**: Schema validation + allowlist  
✅ **Seamless barge-in**: Interrupt TTS anytime  
✅ **Local first**: Privacy-focused, no cloud required  
